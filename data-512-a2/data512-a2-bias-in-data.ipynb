{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing Bias in Detox Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Download datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isdir('data')==False:\n",
    "    os.mkdir('data')\n",
    "if os.path.isdir('images')==False:\n",
    "    os.mkdir('images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_file(url, fname):\n",
    "    urllib.request.urlretrieve(url, fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOXICITY_ANNOTATED_COMMENTS_URL = 'https://ndownloader.figshare.com/files/7394542' \n",
    "TOXICITY_ANNOTATIONS_URL = 'https://ndownloader.figshare.com/files/7394539'\n",
    "TOXICITY_WORKER_DEMOGRAPHICS_IRL = 'https://ndownloader.figshare.com/files/7640581'\n",
    "\n",
    "download_file(TOXICITY_ANNOTATED_COMMENTS_URL, 'data/toxicity_annotated_comments.tsv')\n",
    "download_file(TOXICITY_ANNOTATIONS_URL, 'data/toxicity_annotations.tsv')\n",
    "download_file(TOXICITY_WORKER_DEMOGRAPHICS_IRL, 'data/toxicity_worker_demographics.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ATTACK_ANNOTATED_COMMENTS_URL = 'https://ndownloader.figshare.com/files/7554634' \n",
    "ATTACK_ANNOTATIONS_URL = 'https://ndownloader.figshare.com/files/7554637'\n",
    "ATTACK_WORKER_DEMOGRAPHICS_IRL = 'https://ndownloader.figshare.com/files/7640752'\n",
    "\n",
    "download_file(ATTACK_ANNOTATED_COMMENTS_URL, 'data/attack_annotated_comments.tsv')\n",
    "download_file(ATTACK_ANNOTATIONS_URL, 'data/attack_annotations.tsv')\n",
    "download_file(ATTACK_WORKER_DEMOGRAPHICS_IRL, 'data/attack_worker_demographics.tsv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Load tables using pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "toxicity_annotations = pd.read_csv(\"data/toxicity_annotations.tsv\", delimiter=\"\\t\")\n",
    "toxicity_annotated_comments = pd.read_csv(\"data/toxicity_annotated_comments.tsv\", delimiter=\"\\t\")\n",
    "toxicity_worker_demographics = pd.read_csv(\"data/toxicity_worker_demographics.tsv\", delimiter=\"\\t\")\n",
    "\n",
    "toxicity_worker_demographics = toxicity_worker_demographics.set_index(\"worker_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "attack_annotations = pd.read_csv(\"data/attack_annotations.tsv\", delimiter=\"\\t\")\n",
    "attack_annotated_comments = pd.read_csv(\"data/attack_annotated_comments.tsv\", delimiter=\"\\t\")\n",
    "attack_worker_demographics = pd.read_csv(\"data/attack_worker_demographics.tsv\", delimiter=\"\\t\")\n",
    "\n",
    "attack_worker_demographics = attack_worker_demographics.set_index(\"worker_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "toxicity_annotations = pd.read_csv(\"data/toxicity_annotations.tsv\", delimiter=\"\\t\")\n",
    "toxicity_annotated_comments = pd.read_csv(\"data/toxicity_annotated_comments.tsv\", delimiter=\"\\t\")\n",
    "toxicity_worker_demographics = pd.read_csv(\"data/toxicity_worker_demographics.tsv\", delimiter=\"\\t\")\n",
    "\n",
    "toxicity_worker_demographics = toxicity_worker_demographics.set_index(\"worker_id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1: Analyze the level of disagreement among crowdworkers around certain labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How much do labelers tend to agree while labelling hostile speech? Do people disagree more to a comment more likely to be labeled as hostile? Are some kinds of hostile speech harder for people to agree on than others? For example, do labelers tend to disagree more about “personal attacks” vs. “toxicity”?    \n",
    "\n",
    "For this analysis, we use \"entropy\" to measure the level of disagreement between different workers on the labeling of each comment. Entropy is a measure of randomness in the data which makes it harder to draw any conclusions from. There are several ways to measure entropy. In this analysis, we use [Shannon's entropy](https://arxiv.org/ftp/arxiv/papers/1405/1405.2061.pdf#:~:text=Meaning%20of%20Entropy,of%20information%20in%20that%20variable)  which returns a value between 0 to 1. 0 indicates no entropy i.e. complete agreement while 1 indicates maximum disagreement i.e. 50-50 split between labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 1: We first compute the entropy in the toxicity and personal attacks annotations respectively.  \n",
    "\n",
    "We also find the majority vote (is_toxic/is_attack) for each comment (e.g. is_toxic = 1 if more than half workers label the comment as toxic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Toxicity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "toxicity_mean = toxicity_annotations.groupby(\"rev_id\")[\"toxicity_score\"].mean().to_frame().head().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "toxic_count = pd.DataFrame({\"toxic_count\": toxicity_annotations.groupby(\"rev_id\")[\"toxicity\"].apply(lambda c : c.sum())}).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "nontoxic_count = pd.DataFrame({\"nontoxic_count\": toxicity_annotations.groupby(\"rev_id\")[\"toxicity\"].apply(lambda c : (c == 0).sum())}).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_toxicity_count = toxic_count.set_index(\"rev_id\").join(nontoxic_count.set_index(\"rev_id\")).reset_index()\n",
    "joined_toxicity_count['total_count'] = joined_toxicity_count[\"toxic_count\"] + joined_toxicity_count[\"nontoxic_count\"]\n",
    "joined_toxicity_count['p_toxic'] = joined_toxicity_count['toxic_count']/joined_toxicity_count['total_count']\n",
    "joined_toxicity_count['p_nontoxic'] = joined_toxicity_count['nontoxic_count']/joined_toxicity_count['total_count']\n",
    "joined_toxicity_count[\"entropy\"] = -(np.log2(joined_toxicity_count['p_toxic'])*joined_toxicity_count['p_toxic']) - (np.log2(joined_toxicity_count['p_nontoxic'])*joined_toxicity_count['p_nontoxic'])\n",
    "joined_toxicity_count[\"entropy\"] = joined_toxicity_count[\"entropy\"].fillna(0)\n",
    "joined_toxicity_count['is_toxic'] = joined_toxicity_count['toxic_count']> joined_toxicity_count['nontoxic_count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rev_id</th>\n",
       "      <th>toxic_count</th>\n",
       "      <th>nontoxic_count</th>\n",
       "      <th>total_count</th>\n",
       "      <th>p_toxic</th>\n",
       "      <th>p_nontoxic</th>\n",
       "      <th>entropy</th>\n",
       "      <th>is_toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2232.0</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.468996</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4216.0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8953.0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>26547.0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28959.0</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.721928</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    rev_id  toxic_count  nontoxic_count  total_count  p_toxic  p_nontoxic  \\\n",
       "0   2232.0            1               9           10      0.1         0.9   \n",
       "1   4216.0            0              10           10      0.0         1.0   \n",
       "2   8953.0            0              10           10      0.0         1.0   \n",
       "3  26547.0            0              10           10      0.0         1.0   \n",
       "4  28959.0            2               8           10      0.2         0.8   \n",
       "\n",
       "    entropy  is_toxic  \n",
       "0  0.468996     False  \n",
       "1  0.000000     False  \n",
       "2  0.000000     False  \n",
       "3  0.000000     False  \n",
       "4  0.721928     False  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joined_toxicity_count.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Personal Attacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "attack_count = pd.DataFrame({\"attack_count\": attack_annotations.groupby(\"rev_id\")[\"attack\"].apply(lambda c : c.sum())}).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nonattack_count = pd.DataFrame({\"nonattack_count\": attack_annotations.groupby(\"rev_id\")[\"attack\"].apply(lambda c : (c == 0).sum())}).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_attack_count = attack_count.set_index(\"rev_id\").join(nonattack_count.set_index(\"rev_id\")).reset_index()\n",
    "joined_attack_count['total_count'] = joined_attack_count[\"attack_count\"] + joined_attack_count[\"nonattack_count\"]\n",
    "joined_attack_count['p_attack'] = joined_attack_count['attack_count']/joined_attack_count['total_count']\n",
    "joined_attack_count['p_nonattack'] = joined_attack_count['nonattack_count']/joined_attack_count['total_count']\n",
    "joined_attack_count[\"entropy\"] = -(np.log2(joined_attack_count['p_attack'])*joined_attack_count['p_attack']) - (np.log2(joined_attack_count['p_nonattack'])*joined_attack_count['p_nonattack'])\n",
    "joined_attack_count[\"entropy\"] = joined_attack_count[\"entropy\"].fillna(0)\n",
    "joined_attack_count['is_attack'] = joined_attack_count['attack_count']> joined_attack_count['nonattack_count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_attack_count.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Step 2: Next we compare the distribution of entropy (level of disagreement) between the comment labels for toxicity and personal attacks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toxicity_vs_attack_entropy = joined_attack_count[['rev_id', 'entropy']].merge(joined_toxicity_count[['rev_id', 'entropy']], on = 'rev_id')[['entropy_x', 'entropy_y']]\n",
    "toxicity_vs_attack_entropy.columns = ['attack', 'toxicity']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.melt(toxicity_vs_attack_entropy)\n",
    "fig, ax = plt.subplots(figsize=(15,8))\n",
    "ax.set_title(\"Distribution of Level of Disagreement in Different Kinds of Hostile speech\")\n",
    "sns.violinplot( x=\"variable\", y=\"value\", data=pd.melt(toxicity_vs_attack_entropy), ax=ax, palette = 'magma' )\n",
    "plt.savefig(\"images/label_disagreement_toxic_vs_attack.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that both the distributions and means of the entropy for comments on toxicity and personal attacks are quite similar. This means that people tend to have a similar level of agreement while identifying different types of hostile speech."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Step 3: We compare the entropy between comments which were labelled as hostile by a majority of the labellers vs comments that were labelled as not hostile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first compare the number of hostile to non-hostile comments. We observe that in both datasets (toxicity and personal attacks), there is higher number of comments that are marked as hostile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_toxicity_count.groupby(['is_toxic'])['rev_id'].count().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_attack_count.groupby(['is_attack'])['rev_id'].count().reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Toxicity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_toxicity_count.groupby('is_toxic')['entropy'].mean().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15,8))\n",
    "ax.set_title(\"Distribution of Level of Disagreement in Toxicity annotations, by majority vote\")\n",
    "sns.violinplot( x=\"is_toxic\", y=\"entropy\", data=joined_toxicity_count, ax=ax , palette = 'cubehelix')\n",
    "plt.savefig(\"images/label_disagreement_toxicity.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the mean entropy for comments labeled as not-toxic is lower (0.24) compared to the mean entropy when comments are labeled as toxic (0.53). This means that labelers tend to agree less that a comment is toxic although majority labelers do tend to label it as toxic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Personal Attacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_attack_count.groupby('is_attack')['entropy'].mean().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15,8))\n",
    "ax.set_title(\"Distribution of Level of Disagreement in Personal Attack annotations, by majority vote\")\n",
    "sns.violinplot( x=\"is_attack\", y=\"entropy\", data=joined_attack_count, ax=ax, palette = 'cubehelix' )\n",
    "plt.savefig(\"images/label_disagreement_attack.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that similar to the toxicity data, the mean entropy for comments labeled as not-attacks is lower (0.28) compared to the mean entropy when comments are labeled as attacks (0.58). This means that labelers tend to agree less that a comment is an attack although majority labelers do tend to label it as attack."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results  \n",
    "\n",
    "Labelers tend to agree and disagree similarly for the different kinds of hostile speech (toxicity and personal attack).  \n",
    "A higher mean entropy indicates that there is more disagreement between the labelling of comments that are voted as having hostile speech by a majority of the annotators. This indicates that there is more ambiguity between the decision to label comments as toxic/attack. Thus any model trained on this data might not do a very good job in detecting comments with toxicity or personal attacks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2: Explore relationships between worker demographics and labeling behavior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How consistent are labelling behaviors among workers with different demographic profiles? For example, are female-identified labelers more or less likely to label comments as aggressive than male-identified labelers?\n",
    "\n",
    "For this analysis, we compare the proportion of comments that are marked as toxic between different groups in demographic profiles such as gender, age, education and language. If the data is unbiased we should observe similar proportions between the groups."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 1: Join the annotation data with the worker demographics data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toxicity_worker_demographics = toxicity_worker_demographics.reset_index()\n",
    "attack_worker_demographics = attack_worker_demographics.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toxicity_label_demographics = toxicity_annotations.set_index('worker_id').join(toxicity_worker_demographics.set_index('worker_id'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toxicity_label_demographics.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attack_label_demographics = attack_annotations.set_index('worker_id').join(attack_worker_demographics.set_index('worker_id'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attack_label_demographics.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 2: Find proportion of comments marked as hostile for different demographic profile groups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toxicity_gender_labels = toxicity_label_demographics[toxicity_label_demographics['toxicity'] == 1].groupby('gender')['rev_id'].count().reset_index()\n",
    "toxicity_gender_totals = toxicity_label_demographics.groupby('gender')['rev_id'].count().reset_index()\n",
    "toxicity_gender_labels['proportion'] = toxicity_gender_labels['rev_id']/toxicity_gender_totals['rev_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toxicity_gender_labels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attack_gender_labels = attack_label_demographics[attack_label_demographics['attack'] == 1].groupby('gender')['rev_id'].count().reset_index()\n",
    "attack_gender_totals = attack_label_demographics.groupby('gender')['rev_id'].count().reset_index()\n",
    "attack_gender_labels['proportion'] = attack_gender_labels['rev_id']/attack_gender_totals['rev_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attack_gender_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toxicity_vs_attack_gender = toxicity_gender_labels.merge(attack_gender_labels, on = 'gender')\n",
    "toxicity_vs_attack_gender = pd.DataFrame({'gender': toxicity_vs_attack_gender['gender'], 'toxicity':  toxicity_vs_attack_gender['proportion_x'], 'attack': toxicity_vs_attack_gender['proportion_y']})\n",
    "toxicity_vs_attack_gender = toxicity_vs_attack_gender.set_index('gender').stack().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toxicity_vs_attack_gender.columns = ['gender', 'type', 'comments_proportion']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,5))\n",
    "ax.set_title(\"Proportion of Hostile Comments by Gender\")\n",
    "sns.barplot(x = 'gender', y = 'comments_proportion', hue = 'type', palette = 'magma', data = toxicity_vs_attack_gender)\n",
    "plt.savefig(\"images/hostile_comments_by_gender.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Age Group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toxicity_age_labels = toxicity_label_demographics[toxicity_label_demographics['toxicity'] == 1].groupby('age_group')['rev_id'].count().reset_index()\n",
    "toxicity_age_totals = toxicity_label_demographics.groupby('age_group')['rev_id'].count().reset_index()\n",
    "toxicity_age_labels['proportion'] = toxicity_age_labels['rev_id']/toxicity_age_totals['rev_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toxicity_age_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attack_age_labels = attack_label_demographics[attack_label_demographics['attack'] == 1].groupby('age_group')['rev_id'].count().reset_index()\n",
    "attack_age_totals = attack_label_demographics.groupby('age_group')['rev_id'].count().reset_index()\n",
    "attack_age_labels['proportion'] = attack_age_labels['rev_id']/attack_age_totals['rev_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attack_age_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toxicity_vs_attack_age = toxicity_age_labels.merge(attack_age_labels, on = 'age_group')\n",
    "toxicity_vs_attack_age = pd.DataFrame({'age_group': toxicity_vs_attack_age['age_group'], 'toxicity':  toxicity_vs_attack_age['proportion_x'], 'attack': toxicity_vs_attack_age['proportion_y']})\n",
    "toxicity_vs_attack_age = toxicity_vs_attack_age.set_index('age_group').stack().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,5))\n",
    "ax.set_title(\"Proportion of Hostile Comments by Age Group\")\n",
    "toxicity_vs_attack_age.columns = ['age_group', 'type', 'comments_proportion']\n",
    "sns.barplot(x = 'age_group', y = 'comments_proportion', hue = 'type', palette = 'magma', data = toxicity_vs_attack_age)\n",
    "plt.savefig(\"images/hostile_comments_by_age.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Education"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toxicity_education_labels = toxicity_label_demographics[toxicity_label_demographics['toxicity'] == 1].groupby('education')['rev_id'].count().reset_index()\n",
    "toxicity_education_totals = toxicity_label_demographics.groupby('education')['rev_id'].count().reset_index()\n",
    "toxicity_education_labels['proportion'] = toxicity_education_labels['rev_id']/toxicity_education_totals['rev_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toxicity_education_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attack_education_labels = attack_label_demographics[attack_label_demographics['attack'] == 1].groupby('education')['rev_id'].count().reset_index()\n",
    "attack_education_totals = attack_label_demographics.groupby('education')['rev_id'].count().reset_index()\n",
    "attack_education_labels['proportion'] = attack_education_labels['rev_id']/attack_education_totals['rev_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attack_education_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toxicity_vs_attack_education = toxicity_education_labels.merge(attack_education_labels, on = 'education')\n",
    "toxicity_vs_attack_education = pd.DataFrame({'education': toxicity_vs_attack_education['education'], 'toxicity':  toxicity_vs_attack_education['proportion_x'], 'attack': toxicity_vs_attack_education['proportion_y']})\n",
    "toxicity_vs_attack_education = toxicity_vs_attack_education.set_index('education').stack().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,5))\n",
    "ax.set_title(\"Proportion of Hostile Comments by Education\")\n",
    "toxicity_vs_attack_education.columns = ['education', 'type', 'comments_proportion']\n",
    "sns.barplot(x = 'education', y = 'comments_proportion', hue = 'type', palette = 'magma', data = toxicity_vs_attack_education)\n",
    "plt.savefig(\"images/hostile_comments_by_education.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toxicity_language_labels = toxicity_label_demographics[toxicity_label_demographics['toxicity'] == 1].groupby('english_first_language')['rev_id'].count().reset_index()\n",
    "toxicity_language_totals = toxicity_label_demographics.groupby('english_first_language')['rev_id'].count().reset_index()\n",
    "toxicity_language_labels['proportion'] = toxicity_language_labels['rev_id']/toxicity_language_totals['rev_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toxicity_language_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attack_language_labels = attack_label_demographics[attack_label_demographics['attack'] == 1].groupby('english_first_language')['rev_id'].count().reset_index()\n",
    "attack_language_totals = attack_label_demographics.groupby('english_first_language')['rev_id'].count().reset_index()\n",
    "attack_language_labels['proportion'] = attack_language_labels['rev_id']/attack_language_totals['rev_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attack_language_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toxicity_vs_attack_language = toxicity_language_labels.merge(attack_language_labels, on = 'english_first_language')\n",
    "toxicity_vs_attack_language = pd.DataFrame({'english_first_language': toxicity_vs_attack_language['english_first_language'], 'toxicity':  toxicity_vs_attack_language['proportion_x'], 'attack': toxicity_vs_attack_language['proportion_y']})\n",
    "toxicity_vs_attack_language = toxicity_vs_attack_language.set_index('english_first_language').stack().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toxicity_vs_attack_language.columns = ['english_first_language', 'type', 'comments_proportion']\n",
    "sns.barplot(x = 'english_first_language', y = 'comments_proportion', hue = 'type', palette = 'magma', data = toxicity_vs_attack_language)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results  \n",
    "\n",
    "We observe that there is a similar proportions being tagged as toxic or attack among the different age groups and first language. We see that while the proportion of comments marked as toxic remains similar between different genders, there is an increase in the proportion of comments that workers of 'other' gender label as containing attacks. The possible reason for this could be that the comments annotated for attacks may contain words pertaining to gender or hostile comments for non-binary people.  \n",
    "We also see that workers with no educational background labelled fewer comments as containing personal attacks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Further Implications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q1: Which, if any, of these demo applications would you expect the Perspective API—or any model trained on the Wikipedia Talk corpus—to perform well in? Why?**  \n",
    "1. Comment Blur Filter: (link) Perspective API—or any model trained on the Wikipedia Talk corpus will help in the proper categorisation of the comments into toxic and non-toxic categories. Thus, I would expect them to perform well. Also since the usecase involves free platforms for conversation and no niche scope of discussion, I would expect it to do well.  \n",
    "2. WikiDetox: (link) This is a very good application where Perspective API—or any model trained on the Wikipedia Talk corpus could perform well as the goal matches the intent for creation of API and data  \n",
    "3. Author Perspective for Drupal (link) Here we need to filter the toxic and comments causing harassment. This is a perfect usecase for Perspective API—or any model trained on the Wikipedia Talk corpus  \n",
    "\n",
    "**Q2:Which, if any, of these demo applications would you expect the Perspective API to perform poorly in? Why?**  \n",
    "1. Toxicity Timeline: (link)It would perform poorly because time is not given to the required accuracy in Wikipedia corpus  \n",
    "2. An authorship experience demo for Perspective API: (link) It Perspective API may not perform well as this is a feedback data. Comments are generally free form and do not have a theme. Also, people are more likely to comment on anything they like or not, but mostly are biased to give a feedback only when the feedback is negative.  \n",
    "3. Hot Topics (link): The data covered in Wikipedia talk corpus is comment based. This demo checks the whole of the document. The Perspective API may or may not be able to perform well here  \n",
    "\n",
    "**Q3:What are some kinds of hostile speech that would be difficult to accurately detect using the approach used to train the Perspective API models?**  \n",
    "1. Any comments not in English would be difficult to accurately detect using the approach used to train the Perspective API models. This would be because the model would be trained in English, and if the model encounters any negative word, it may not understand that it is toxic/aggresive, due to change in language/font etc.\n",
    "2. Sarcasm could be difficult to detect with these models\n",
    "3. Any data other than comment-like data may not perform well here. For example- it would be difficult to detect a lenghty toxic news article because comments are generally the length of 1-10 sentences."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
